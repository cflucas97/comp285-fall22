 \documentclass [12pt]{article} 

\usepackage {amsmath}
\usepackage {amsthm}
\usepackage {amssymb}
\usepackage {graphicx} 
\usepackage {float}
\usepackage {multirow}
\usepackage {xcolor}
\usepackage {algorithmic}
\usepackage [ruled,vlined,commentsnumbered,titlenotnumbered]{algorithm2e} \usepackage {array} 
\usepackage {booktabs} 
\usepackage {url} 
\usepackage {parskip} 
\usepackage [margin=1in]{geometry} 
\usepackage [T1]{fontenc} 
\usepackage {cmbright} 
\usepackage [many]{tcolorbox} 
\usepackage [colorlinks = true,
            linkcolor = blue,
            urlcolor  = blue,
            citecolor = blue,
            anchorcolor = blue]{hyperref} 
\usepackage {enumitem} 
\usepackage {xparse} 
\usepackage {verbatim}
\usepackage{listings}
\usepackage{xcolor}
\lstset { %
    language=C++,
    backgroundcolor=\color{black!5}, % set backgroundcolor
    basicstyle=\footnotesize,% basic font setting
}
\newtheorem{theorem}{Theorem}
\newtheorem{remark}{Remark}



\DeclareTColorBox {Solution}{}{breakable, title={Solution}} \DeclareTColorBox {Solution*}{}{breakable, title={Solution (provided)}} \DeclareTColorBox {Instruction}{}{boxrule=0pt, boxsep=0pt, left=0.5em, right=0.5em, top=0.5em, bottom=0.5em, arc=0pt, toprule=1pt, bottomrule=1pt} \DeclareDocumentCommand {\Expecting }{+m}{\textbf {[We are expecting:} #1\textbf {]}} \DeclareDocumentCommand {\Points }{m}{\textbf {(#1 pt.)}} 

\begin {document} 

\vspace {1em} 
\begin {Instruction} 
Adapted From Virginia Williams' lecture notes.
\end {Instruction}  

{\LARGE \textbf {COMP 285 (NC A\&T, Spr `22)}\hfill \textbf {Lecture 9} } 

\begin{centering}
\section*{Randomized Algorithms and QuickSort}
\end{centering}

Today we will study another sorting algorithm: \texttt{Quicksort}, which was invented in 1959 by Tony
Hoare. You may wonder why we want to study a new sorting algorithm. We have already studied \texttt{MergeSort}, which we showed to perform significantly better than the trivial $O(n^2)$ algorithm. While \texttt{MergeSort} achieves an $O(n \log n)$ worst-case asymptotic bound, in practice,
there are a number of implementation details about \texttt{MergeSort} that make it tricky to achieve
high performance. \texttt{Quicksort} is an alternative algorithm, which is simpler to implement in
practice. \texttt{Quicksort} will also use a divide and conquer strategy but will use randomization to
improve the performance of the algortithm in expectation. Java, Unix, and C stdlib all have
implementations of \texttt{Quicksort} as one of their built-in sorting routines.


\section{QuickSort Overview}
As in all sorting algorithms, we start with an array $A$ of $n$ numbers; again we assume without
loss of generality that the numbers are distinct\footnote{What does it mean to say that we can assume something without loss of generality? Why can we make this assumption without loss of generality in this proof?}. Quicksort is very similar to the \texttt{QuickSort} algorithm we studied last lecture. The description of Quicksort is the following:

\begin{algorithm}
\caption{QuickSort(A)}\label{alg:QuickSort}
\begin{algorithmic}
\IF {$n=1$}
    \RETURN $A$
\ENDIF
\STATE $p \gets \texttt{RandomPivot}(A)$
\STATE $A_< \gets \{A[i] \mid A[i] < p \}$
\STATE $A_> \gets \{A[i] \mid A[i] > p \}$
\STATE $A_<' \gets \texttt{QuickSort}(A_<)$
\STATE $A_>' \gets \texttt{QuickSort}(A_>)$
\RETURN $[A_<', p, A_>']$
\end{algorithmic}
\end{algorithm}

The above steps define a ``partition'' function on $A$. The partition function of \texttt{Quicksort} can vary depending on how the pivot is chosen and also on the implementation. \texttt{Quicksort} is often used in practice because we can implement this step in linear time, and with very small constant factors. In addition, the rearrangement can actually be done in-place, rather
than making several copies of the array (as is done in \texttt{MergeSort}). In these notes we will
not describe the details of an in-place implementation, but the pseudocode can be found in
CLRS.


\section{Speculation on the Runtime}

The performance of \texttt{Quicksort} depends on which element is chosen as the pivot. Assume
that we choose the $k$th smallest element; then $|A_<| = k - 1, |A_>| = n - k$. This allows us to write a recurrence; let $T(n)$ be the runtime of \texttt{Quicksort} on an $n$-element array. We know the partition step takes $O(n)$ time; therefore the recurrence is

$$
T(n) \leq cn + T(k-1) + T(n-k)
$$

For the worst pivot choice (the maximum or minimum element in the array\footnote{2Why are these the worst pivot choices?}), the runtime satisfies the recurrence $T(n) = T(n - 1) + O(n)$; hence $T(n) = O(n^2)$.

One way that seems optimal to define the partition function is to pick the median as the
pivot. In the above recurrence this would mean that $k = \lceil \frac{n}{2}\rceil$. We showed in the previous lecture that the algorithm \texttt{Select} can find the median element in linear time. Therefore the recurrence becomes $T(n) \leq cn + 2T(n^2)$. This is exactly the same recurrence as \texttt{MergeSort}, which means that this algorithm is guaranteed to run in $O(n \log n)$ time.

Unfortunately, the median selection algorithm is not practical; while it runs in linear time, it
has much larger constant factors than we would like. To improve it, we will explore some
alternative methods for choosing a pivot.

We leave the proof of correctness of \texttt{Quicksort} as an exercise to the reader (hint: use
induction).


\section{Random Pivot Selection}
Our discussion so far suggests two approaches to pivot selection: (1) We could pick an
arbitrary element as a pivot. This is super simple and takes O(1) time (good), but the worst
case running time of the resulting algorithm would be $O(n^2)$ (bad). (2) We could pick the
median as the pivot. This takes $O(n)$ time and is somewhat complicated (bad), but gives a
worst case running time of $O(n \log n)$ (good). Can we get the best of both worlds?

One method of ``defending'' against making a bad pivot choice is to choose a random element
as the pivot. We observe that it is unlikely that the random element will be either the
median (best-case) or the maximum or minimum (worst-case). Note that we have a uniform
distribution over the $n$ order statistics of the array (this is a fancy way of saying: for every
$1 \leq i \leq n$ we pick the $i$-th highest element with the same probability $\frac{1}{n}$). Let's pause for a moment. This is the first time in this course that we have encountered an algorithm that uses randomness in its execution. Such an algorithm (that ``flips coins'' and takes actions based on the outcome of these coin flips) is called a randomized algorithm. How do we analyze a randomized algorithm?

\section{Worst-Case Analysis}
In this section we will derive a bound on the worst-case running time of \texttt{Quicksort}. If we
consider the worst random choice of pivot at each step, the running time will be $\Theta(n^2)$. This flavor of worst-case analysis (which gets an upper bound on the running time over all possible possible choices of pivots) is no different from the worst case analysis of the algorithm which picks an arbitrary pivot at every step. We are thus interested in what the running time of \texttt{Quicksort} is \textbf{on average over all possible choices of the pivots}. We should emphasize an important point: We still consider the running time for a \textbf{worst-case input}, and average only \textbf{over the random choices of the algorithm} (which is different from averaging over all possible inputs). Put differently, our analysis will guarantee that \textbf{for any input, the expected running time will be small}. We formalize the idea of averaging over the random choices of the algorithm by considering the running time of the algorithm on an input I as a random variable and bounding the expectation of that random variable.


\textbf{Proposition}. \textit{For every input array of size $n$, the expected running time of \texttt{QuickSort} is$O(n \log n)$}.

Recall that a random variable is a function that maps every element in the sample space to a real number. In the case of rolling a die, the real number (or value of the point in the sample space) would be the number on the top of the die. Here the sample space is the set of all possible choices of pivots, and an example for a random variable can be the running time of \texttt{Quicksort} on a specific input $I$.

Denote by $z_i$ the $i$-th element in the sorted array. For each $i, j$, we define a random variable $X_{i ,j} (\sigma)$ to be the number of times $z_i$ and $z_j$ are compared for a given series of pivot choices $\sigma$. What are the possible values for $X_{i ,j} (\sigma)$? It can be $0$ if $z_i$ and $z_j$ are not compared. Note that all comparisons are with the pivot, and that the pivot is not included in the elements of the arrays in the recursive calls. If $z_i$ and $z_j$ are compared, consider the first time that this happens: one of them must be the pivot at this stage and this pivot is excluded from the subarrays that recursive calls operate on. Thus, \textit{no two elements are compared twice}. Therefore, $X_{i ,j} (\sigma) \in \{0, 1\}$. 

Our goal is to compute the expected number of comparisons that \texttt{Quicksort} makes. Recall the definition of expectation:

$$
\mathbb{E}[X] = \sum_{\sigma} \mathbb{P}[\sigma]X(\sigma) = \sum_{k} k\mathbb{P}[X = k]
$$

Unfortunately for us, this definition does not really give us any clue about how we can actually go about computing the expectation of the complicated random variable that we have at hand, i.e. the number of comparisons made by \texttt{Quicksort}. Can you imagine trying to figure out the probability that \texttt{Quicksort} actually performs exactly $k$ comparisons? We donâ€™t have the foggiest idea how we might do that. If you do, please let us know!

Luckily for us, there is way around this. An important property of expectation is \textbf{linearity of expectation}. For any random variables $X_1, \cdots, X_n$:
$$
\mathbb{E}\left[\sum_{i=1}^n X_i \right] = \sum_{i=1}^n \mathbb{E}[X_i]
$$

This is a really simple to state, but amazingly useful property that makes the computation of expectations of seemingly complicated random variables much much easier. It is worth internalizing this technique that we are about to apply for analyzing the expected number of comparisons that \texttt{Quicksort} performs.

We start with computing the expected value of $X_{i ,j}$. These variables are indicator random variables, which take the value $1$ if some event happens, and $0$ otherwise. The expected value is:
\begin{align*}
\mathbb{E}[X_{i,j}] &= \mathbb{P}[X_{i,j} = 1] \cdot 1 + \mathbb{P}[X_{i,j} = 0] \cdot 0 \tag{definition of expectation} \\
&= \mathbb{P}[X_{i,j} = 1]
\end{align*}

Let $C(\sigma)$ be the total number of comparisons made by \texttt{Quicksort} for a given set of pivot choices $\sigma$:

$$
C(\sigma) = \sum_{i=1}^n \sum_{j=i+1}^n X_{i,j}(\sigma) 
$$

We wish to compute $\mathbb{E}[C]$ to get the expected number of comparisons made by \texttt{Quicksort}
for an input array of size $n$.

\begin{align*}
  \mathbb{E}[C] &= \mathbb{E}\left[\sum_{i=1}^n \sum_{j=i+1}^n X_{i,j}(\sigma) \right] \tag{definition of $C$} \\
  &= \sum_{i=1}^n \mathbb{E}\left[\sum_{j=i+1}^n X_{i,j}(\sigma) \right] \tag{linearity of expectation on first sum} \\
  &= \sum_{i=1}^n \sum_{j=i+1}^n  \mathbb{E}[X_{i,j}(\sigma)] \tag{linearity of expectation on second sum} \\
  &= \sum_{i=1}^n \sum_{j=i+1}^n  \mathbb{P}[z_i, z_j \text{ are compared} ]  \tag{definition of expectation on indicator variable}\\
\end{align*}
Now we find $P[z_i, z_j \text{ are compared}]$. Apriori, this seems difficult to do, but it turns out that there is a really elegant way to analyze this. Note that each element in the array(except the pivot) is compared to the pivot at each level of the recurrence. To analyze $P[z_i, z_j \text{ are compared}]$, examine the portion of the array $[z_i, \cdots, z_j]$. After the array is split using a pivot from $[z_i, \cdots, z_j]$, $z_i$ and $z_j$ can no longer be compared. Hence, $z_i$ and $z_j$ are compared only when from the portion of the array $[z_i, \cdots, z_j]$, either $z_i$ or $z_j$ is the first one picked as the pivot. So,

\begin{align*}
\mathbb{P}[z_i, z_j \text{ are compared}] &= \mathbb{P}[z_i \text{ or } z_j \text{ is the first pivot picked from } [z_i, \cdots, z_j]] \tag{argued above} \\
&= \frac{1}{j-i+1} + \frac{1}{j - i + 1} \tag{picking from a set uniformly} \\
&= \frac{2}{j-i+1}
\end{align*}

Make sure you are able to explain the second line in the calculation above.

We return to the expected value of $C$:

\begin{align*}
\mathbb{E}[C] &= \sum_{i=1}^n \sum_{j = i + 1}^n \mathbb{P}[z_i, z_j \text{ are compared }] \\
&= \sum_{i=1}^n \sum_{j = i + 1}^n \frac{2}{j-i + 1}
\end{align*}
Note that for a fixed $i$,
\begin{align*}
  \sum_{j=i+1}^n \frac{1}{j-i+1} &= \frac{1}{2} + \frac{1}{3} + \cdots + \frac{1}{n - i + 1} \\
  &\leq \frac{1}{2} + \frac{1}{3} + \cdots + \frac{1}{n}
\end{align*}

And using $\sum_{k=2}^n \frac{1}{k} \leq \ln n$, we get that

\begin{align*}
\mathbb{E}[C] &= \mathbb{E}\left[\sum_{i=1}^n \sum_{j = i + 1}^n X_{i,j}(\sigma) \right] \\
&= \sum_{i=1}^n \sum_{j = i + 1}^n \frac{2}{j-i + 1}\\
&\leq 2n \ln n
\end{align*}
 
 Thus, the expected number of comparisons made by \texttt{Quicksort} is no greater than $2n \ln n = O(n \log n)$. To complete the proof, we have to show that the running time is dominated by the number of comparisons. Note that in each recursive call to \texttt{Quicksort} on an array of size $k$, the algorithm performs $k - 1$ comparisons in order to split the array, and the amount of work done is $O(k)$. In addition, \texttt{Quicksort} will be called on single-element arrays at most once for each element in the original array, so the total running time of \texttt{Quicksort} $is O(C + n)$. In conclusion, the expected running time of \texttt{Quicksort} on worst-case input is $O(n log n)$.

 \subsection{Alternative Proof}

Here we provide an alternative method for bounding the expected number of comparisons. Let $T(n)$ be the expected number of comparisons performed by \texttt{Quicksort} on an input of size $n$. In general, if the pivot is chosen to be the $i$-th order statistic of the input array (i.e. the $i$th largest element),

$$
T(n) = n - 1 + T(i - 1) + T(n - i)
$$

where we define $T(0) = 0$. Each of the $n$ possible choices of $i$ are equally likely. Thus, the
expected number of comparisons is:

\begin{align*}
T(n) &= n - 1 + \frac{1}{n} \sum_{i=1}^n \left(T(i - 1) + T(n - i)\right) \\
&= n - 1 + \frac{2}{n} \sum_{i=1}^{n-1} T(i) \tag{Each $T(i)$ shows up twice in the sum}
\end{align*}

Did you catch this minor detail: why does the second summation go from $1$ through $n - 1$, while on the previous line, the summation goes from $1$ through $n$?

Continuing the calculation, we will use two facts:

\begin{enumerate}
  \item $\sum_{i=1}^{n-1} f(i) \leq \int_1^n f(x) dx$ for an increasing function $f$.
  (How would you prove this? This gives us a way to upper-bound discrete sums by integrals of continuous functions â€“ a nice trick to have in your toolkit that we will use below. If you understand this, what is the corresponding statement to lower-bound a discrete sum by an integral? What if $f$ was a decreasing function of $n$? It is good to get into the habit of asking yourself such questions to test your understanding.)
  \item $\int 2x \ln x dx = x^2 \ln x - \frac{x^2}{2} + C$
\end{enumerate}

Now we show that $T(n) \leq 2n \ln n$ by (strong) induction.

\textbf{Inductive Hypothesis.} $T(i) \leq 2i \ln i$.

\textbf{Base case (i = 1)} An array of size 1 requires no comparisons. Thus, $T(1) = 0$ and the inductive hypothis is true for $i = 1$.

\textbf{Inductive step}. We will show that if the inductive hypothesis is true for all $i \leq k - 1$ then
the inductive hypothesis is also true for $i = k$.

Let's bound $T(k)$

\begin{align*}
T(k) &= k - 1 + \frac{2}{k}\sum_{i=1}^{k-1}T(i) \tag{by definition} \\
&\leq k - 1 + \frac{2}{k}\sum_{i=1}^{k-1}2i \ln i \tag{using our strong inductive hypothesis} \\
&\leq k - 1 + \frac{2}{k}\int_{1}^k (2x \ln x) dx \tag{Using fact 1 above} \\
&= k - 1 + \frac{2}{k}\left[k^2 \ln k - \frac{k^2}{2} + \frac{1}{2} \right] \tag{Using fact 2 above} \\
&= 2k \ln k + k - 1 - k + \frac{1}{k} \tag{simplify} \\
&= 2k \ln k - 1 + \frac{1}{k} \\
&\leq 2k \ln k
\end{align*}

Thus the inductive hypothesis is true for $i = k$. This establishes the inductive step.

\textbf{Conclusion} By induction, we have proved that $T(i) \leq 2i \ln i$ for all $i$. Hence $T(n) \leq
2n \ln n$. This concludes the proof.






























\end{document}