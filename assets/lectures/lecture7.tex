 \documentclass [12pt]{article} 

\usepackage {amsmath}
\usepackage {amsthm}
\usepackage {amssymb}
\usepackage {graphicx} 
\usepackage {float}
\usepackage {multirow}
\usepackage {xcolor}
\usepackage {algorithmic}
\usepackage [ruled,vlined,commentsnumbered,titlenotnumbered]{algorithm2e} \usepackage {array} 
\usepackage {booktabs} 
\usepackage {url} 
\usepackage {parskip} 
\usepackage [margin=1in]{geometry} 
\usepackage [T1]{fontenc} 
\usepackage {cmbright} 
\usepackage [many]{tcolorbox} 
\usepackage [colorlinks = true,
            linkcolor = blue,
            urlcolor  = blue,
            citecolor = blue,
            anchorcolor = blue]{hyperref} 
\usepackage {enumitem} 
\usepackage {xparse} 
\usepackage {verbatim}
\usepackage{listings}
\usepackage{xcolor}
\lstset { %
    language=C++,
    backgroundcolor=\color{black!5}, % set backgroundcolor
    basicstyle=\footnotesize,% basic font setting
}
\newtheorem{theorem}{Theorem}
\newtheorem{remark}{Remark}



\DeclareTColorBox {Solution}{}{breakable, title={Solution}} \DeclareTColorBox {Solution*}{}{breakable, title={Solution (provided)}} \DeclareTColorBox {Instruction}{}{boxrule=0pt, boxsep=0pt, left=0.5em, right=0.5em, top=0.5em, bottom=0.5em, arc=0pt, toprule=1pt, bottomrule=1pt} \DeclareDocumentCommand {\Expecting }{+m}{\textbf {[We are expecting:} #1\textbf {]}} \DeclareDocumentCommand {\Points }{m}{\textbf {(#1 pt.)}} 

\begin {document} 

{\LARGE \textbf {COMP 285 (NC A\&T, Spr `22)}\hfill \textbf {Lecture 7} } 
\vspace {1em} 
\begin {Instruction} 

Adapted From Virginia Williams' lecture notes. Additional credits: J. Su, W. Yang, Gregory Valiant, Mary Wootters, Aviad Rubinstein, Sami Alsheikh.
\end {Instruction} 

\begin{centering}
\section*{k-Select Problem \& Median Selection}
\end{centering}

\section{Introduction}
In the last lecture, we wrapped up with the subtitution method for solving recurrences and introduced the selection problem. In this lecture, we find an $O(n)$ algorithm to solve the selection problem.


\section{Selection Problem}
The selection problem is to find the $k$-th smallest number in an array $A$.

\textbf{Input}: array $A$ of $n$ numbers, and an integer $k \in \{1, \cdot , n\}$.
\textbf{Output}: the $k$-th smallest number in $A$.

One approach is to sort the numbers in ascending order, and then return the $k$th number in the sorted list. This takes $O(n \log n)$ time, since it takes $O(n \log n)$ time for the sort (e.g. by \texttt{MergeSort}) and $O(1)$ time to return $k$th number.

\subsection{Minimum Element}
As always, we ask if we can do better (i.e. faster in big-O terms). In the special case where $k = 1$, selection is the problem of finding the minimum element. We can do this in $O(n)$ time by scanning through the array and keeping track of the minimum element so far. If the current element is smaller than the minimum so far, we update the minimum.

\begin{algorithm}
\caption{SelectMin(A)}\label{alg:min}
\begin{algorithmic}
\STATE $m \gets \infty$
\STATE $n \gets $ length($A$)
\FOR{$i=1$ to $n$}
    \IF{$A[i] < m$}
        \STATE $m \gets A[i]$
    \ENDIF
\ENDFOR
\end{algorithmic}
\end{algorithm}

In fact, this is the best running time we could hope for.

\textbf{Definition.} A deterministic algorithm is one which, given a fixed input, always performs the same operations (as opposed to an algorithm which uses randomness).

\textbf{Claim.} \textit{Any deterministic algorithm for finding the minimum has runtime $\Omega(n)$.}

\begin{proof}
Intuitively, the claim holds because any algorithm for the minimum must
look at all the elements, each of which could be the minimum. Suppose a correct deterministic algorithm does not look at $A[i]$ for some $i$. Then the output cannot depend on $A[i]$, so the algorithm returns the same value whether $A[i]$ is the minimum element or the maximum element. Therefore the algorithm is not always correct, which is a contradiction. So there is no sublinear deterministic algorithm for finding the minimum.
\end{proof}

So for $k = 1$, we have an algorithm which achieves the best running time possible. By similar reasoning, this lower bound of $\Omega(n)$ applies to the general selection problem. So ideally we would like to have a linear-time selection algorithm in the general case.

\section{Linear-Time Selection}
In fact, a linear-time selection algorithm does exist. Before showing the linear time selection algorithm, it's helpful to build some intuition on how to approach the problem. The high-level idea will be to try to do a Binary Search over an unsorted input. At each step, we hope to divide the input into two parts, the subset of smaller elements of $A$, and the subset of larger elements of $A$. We will then determine whether the $k$-th smallest element lies in the first part (with the ``smaller'' elements) or the part with larger elements, and recurse on exactly one of
those two parts.

How do we decide how to partition the array into these two pieces? Suppose we have a
black-box algorithm \texttt{ChoosePivot} that chooses some element in the array $A$, and we use this pivot to define the two setsâ€“any $A[i]$ less than the pivot is in the set of ``smaller'' values, and any $A[i]$ greater than the pivot is in the other part. We will figure out precisely how to specify this subroutine \texttt{ChoosePivot} a bit later, after specifying the high-level algorithm structure. The algorithm \texttt{ChoosePivot} does not affect the \textit{correctness} of the algorithm as we will see in \ref{alg:select}. Rather, it only affects the runtime.

For clarity we'll assume all elements are distinct from now on, but the idea generalizes easily. Let $n$ be the size of the array and assume we are trying to find the $k$-th element.

\begin{algorithm}
\caption{Select(A, n, k)}\label{alg:select}
\begin{algorithmic}
\IF {$n=1$}
    \RETURN $A[1]$
\ENDIF
\STATE $p \gets \texttt{ChoosePivot}(A, n)$
\STATE $A_< \gets \{A[i] \mid A[i] < p \}$
\STATE $A_> \gets \{A[i] \mid A[i] > p \}$
\IF {$|A_<| = k - 1$}
    \RETURN $p$
\ELSIF {$|A_<| > k - 1$}
    \RETURN \texttt{Select}$(A_<, |A_<|, k)$
\ELSIF {$|A_<| < k - 1$}
    \RETURN \texttt{Select}$(A_>, |A_>|, k - |A_<| - 1)$
\ENDIF
\end{algorithmic}
\end{algorithm}

At each iteration, we use the element $p$ to partition the array into two parts: all elements smaller than the pivot and all elements larger than the pivot, which we denote $A_<$ and $A_>$, respectively.

Depending on what the size of the resulting sub-arrays are, the runtime can be different. For example, if one of these sub-arrays is of size $n - 1$, at each iteration, we only decreased the size of the problem by 1, resulting in total running time $O(n^2)$. If the array is split into two equal parts, then the size of the problem at iteration reduces by half, resulting in a linear time solution. (We assume \texttt{ChoosePivot} runs in $O(n)$.)

\textbf{Proposition.} \textit{ If the pivot p is chosen to be the minimum or maximum element, then Select runs in $\Theta(n^2)$ time.}

\textit{Proof.} At each iteration, the number of elements decreases by $1$. Since running \texttt{ChoosePivot} and creating $A_<$ and $A_>$ takes linear time, the recurrence for the runtime is $T(n) = T(n - 1) + \Theta(n)$. Expanding this,
$$
T(n) \leq c_1n + c_1(n -  1) + c_1(n -  2) + ... + c_1 = c_1n(n + 1)/2
$$
and
$$
T(n) \geq c_2n + c_2(n -  1) + c_2(n -  2) + ... + c_2 = c_2n(n + 1)/2.
$$
We conclude that $T(n) = \Theta(n^2)$.

\textbf{Proposition}. \textit{If the pivot $p$ is chosen to be the median element, then Select runs in $O(n)$ time.}

\textit{Proof}. Intuitively, the running time is linear since we remove half of the elements from consideration each iteration. Formally, each recursive call is made on inputs of half the size, namely, $T(n) \leq T(n/2)+cn$. Expanding this, the runtime is $T(n) \leq cn+cn/2+cn/4+...+c \leq 2cn$, which is $O(n)$. So how do we design \texttt{ChoosePivot} that chooses a pivot in linear time? In the following, we describe three ideas.

\subsection{Idea \#1: Choose a random pivot}

As we saw earlier, depending on the pivot chosen, the worst-case runtime can be $O(n^2)$ if we are unlucky in the choice of the pivot at every iteration. As you might expect, it is extremely unlikely to be this unlucky, and one can prove that the expected runtime is $O(n)$ provided the pivot is chosen uniformly at random from the set of elements of $A$. In practice, this randomized algorithm is what is implemented, and the hidden constant in the $O(n)$ runtime is very small.

\subsection{Idea \#2: Choose a pivot that creates the most ``balanced'' split}

Consider \texttt{ChoosePivot} that returns the pivot that creates the most ``balanced'' split, which would be the median of the array. However, this is exactly selection problem we are trying to solve, with $k = n/2$! As long as we do not know how to find the median in linear time, we cannot use this procedure as \texttt{ChoosePivot}.

\end{document}